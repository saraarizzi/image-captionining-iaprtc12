{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3VFsjecPbUky"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import zipfile\n","from shutil import copyfile\n","from IPython.display import clear_output\n","from PIL import Image\n","from nltk.translate.bleu_score import sentence_bleu\n","from textwrap import wrap\n","from contextlib import redirect_stdout\n","\n","from google.colab import drive\n","\n","import tensorflow as tf\n","from keras import regularizers\n","from keras.layers import TextVectorization, Input, Dropout, Dense, Embedding, LSTM, add\n","from keras.utils import to_categorical, pad_sequences, Sequence, plot_model, load_img, img_to_array\n","from keras.models import Model, load_model\n","from keras.callbacks import Callback\n","from keras.losses import CategoricalCrossentropy\n","from keras.optimizers import Adam\n","from keras.initializers import Constant\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17337,"status":"ok","timestamp":1687192894565,"user":{"displayName":"Sara Arizzi","userId":"02819317228817662395"},"user_tz":-120},"id":"5jgDe1QQbzKS","outputId":"f0d0479a-ea30-4c6d-ca56-7e2ffbe1f9b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["#Â mount Google Drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNvoSm6acDzG"},"outputs":[],"source":["GDRIVE_WORKING_PATH = \"/content/gdrive/MyDrive/image_cap\""]},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"zQMWYuEro8tb"}},{"cell_type":"markdown","source":["\n","## Read pre-saved files"],"metadata":{"id":"jBO90z2WecaT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoNL5IhPBeJG"},"outputs":[],"source":["with open(f\"{GDRIVE_WORKING_PATH}/list_id_train.pkl\", \"rb\") as f:\n","  train_ids = pickle.load(f)\n","with open(f\"{GDRIVE_WORKING_PATH}/list_id_test.pkl\", \"rb\") as f:\n","  test_ids = pickle.load(f)\n","with open(f\"{GDRIVE_WORKING_PATH}/list_id_val.pkl\", \"rb\") as f:\n","  val_ids = pickle.load(f)"]},{"cell_type":"code","source":["with open(f\"{GDRIVE_WORKING_PATH}/mapping_id_caption.pkl\", \"rb\") as f:\n","  mapping_id_caption = pickle.load(f)"],"metadata":{"id":"-QGDlDuF-0_q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Read Vectorizers"],"metadata":{"id":"BwEDnS6pcJie"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiVC_2MbBKdC","executionInfo":{"status":"ok","timestamp":1687192897964,"user_tz":-120,"elapsed":3403,"user":{"displayName":"Sara Arizzi","userId":"02819317228817662395"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"83d2c115-08dc-4f0c-f18e-541d29d23c2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4159"]},"metadata":{},"execution_count":6}],"source":["def get_vectorizer():\n","  from_disk = pickle.load(open(f\"{GDRIVE_WORKING_PATH}/vectorizer.pkl\", \"rb\"))\n","  new_v = TextVectorization.from_config(from_disk.get(\"config\"))\n","  new_v.set_weights(from_disk.get(\"weights\"))\n","  return new_v\n","\n","vectorizer = get_vectorizer()\n","VOCAB_SIZE = len(vectorizer.get_vocabulary())\n","VOCAB_SIZE"]},{"cell_type":"code","source":["def get_vectorizer_aug():\n","  from_disk = pickle.load(open(f\"{GDRIVE_WORKING_PATH}/vectorizer_augmented.pkl\", \"rb\"))\n","  new_v = TextVectorization.from_config(from_disk.get(\"config\"))\n","  new_v.set_weights(from_disk.get(\"weights\"))\n","  return new_v\n","\n","vectorizer_aug = get_vectorizer_aug()\n","VOCAB_SIZE_AUG = len(vectorizer_aug.get_vocabulary())\n","VOCAB_SIZE_AUG"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzQJA8od7fUA","executionInfo":{"status":"ok","timestamp":1687192898491,"user_tz":-120,"elapsed":531,"user":{"displayName":"Sara Arizzi","userId":"02819317228817662395"}},"outputId":"079e5bf5-8191-4a56-d341-78acc4e87393"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7557"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Read Embedding Layers"],"metadata":{"id":"z8kjU4AJcRg-"}},{"cell_type":"code","source":["def get_embedding_layer():\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/embedding_matrix.pkl\", \"rb\") as f:\n","    embedding_matrix = pickle.load(f)\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/embedding_layer.pkl\", \"rb\") as f:\n","    embedding_layer = pickle.load(f)\n","\n","  return embedding_layer"],"metadata":{"id":"1ffU0twC3TFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_embedding_layer_aug():\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/embedding_matrix_augmented.pkl\", \"rb\") as f:\n","    embedding_matrix = pickle.load(f)\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/embedding_layer_augmented.pkl\", \"rb\") as f:\n","    embedding_layer = pickle.load(f)\n","\n","  return embedding_layer"],"metadata":{"id":"IZ1GldON7bH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bSp1JdKbYEf"},"outputs":[],"source":["def get_vectorized_caption():\n","  with open(f\"{GDRIVE_WORKING_PATH}/vectorized_captions_train.pkl\", \"rb\") as f:\n","    enc_train = pickle.load(f)\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/vectorized_captions_test.pkl\", \"rb\") as f:\n","    enc_test = pickle.load(f)\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/vectorized_captions_val.pkl\", \"rb\") as f:\n","    enc_val = pickle.load(f)\n","\n","  return enc_train, enc_test, enc_val"]},{"cell_type":"code","source":["def get_vectorized_caption_train_aug():\n","  with open(f\"{GDRIVE_WORKING_PATH}/vectorized_aug_captions_train.pkl\", \"rb\") as f:\n","    enc_train = np.array(pickle.load(f))\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/vectorized_aug_captions_val.pkl\", \"rb\") as f:\n","    enc_val = np.array(pickle.load(f))\n","\n","  return enc_train, enc_val"],"metadata":{"id":"MmxbjuNJ_8GB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Read IMG Features"],"metadata":{"id":"CdiGZKEDcYqW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9V-_f-qbP-g"},"outputs":[],"source":["def get_features_vgg():\n","  with open(f\"{GDRIVE_WORKING_PATH}/dict_features_train_vgg16.pkl\", \"rb\") as f:\n","    img_features_train = pickle.load(f)\n","  img_train = np.squeeze(np.array(list(img_features_train.values())))\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/dict_features_test_vgg16.pkl\", \"rb\") as f:\n","    img_features_test = pickle.load(f)\n","  img_test = np.squeeze(np.array(list(img_features_test.values())))\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/dict_features_val_vgg16.pkl\", \"rb\") as f:\n","    img_features_val = pickle.load(f)\n","  img_val = np.squeeze(np.array(list(img_features_val.values())))\n","\n","  return img_train, img_val, img_test"]},{"cell_type":"code","source":["def get_features_vgg_places():\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/dict_features_train_vgg16_places.pkl\", \"rb\") as f:\n","    img_features_train = pickle.load(f)\n","  img_train = np.squeeze(np.array(list(img_features_train.values())))\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/dict_features_test_vgg16_places.pkl\", \"rb\") as f:\n","    img_features_test = pickle.load(f)\n","  img_test = np.squeeze(np.array(list(img_features_test.values())))\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/dict_features_val_vgg16_places.pkl\", \"rb\") as f:\n","    img_features_val = pickle.load(f)\n","  img_val = np.squeeze(np.array(list(img_features_val.values())))\n","\n","  return img_train, img_val, img_test"],"metadata":{"id":"aBYNQzyfcboU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Generator"],"metadata":{"id":"293osBBwonr7"}},{"cell_type":"markdown","source":["### For one single caption per image"],"metadata":{"id":"SrFAy9l_nw0p"}},{"cell_type":"code","source":["class DataGen(Sequence):\n","\n","  def __init__(self, img_features, cap_encoded, batch_size, vocab_size, max_length):\n","\n","    self.img = img_features\n","    self.cap = cap_encoded\n","    self.batch_size = batch_size\n","    self.vocab_size = vocab_size\n","    self.max_length = max_length\n","    self.n = len(self.img)\n","\n","  def __len__(self):\n","    return self.n // self.batch_size\n","\n","  def __getitem__(self, idx):\n","    low = idx * self.batch_size\n","    high = min(low + self.batch_size, self.n)\n","    batch_x_img = self.img[low:high]\n","    batch_x_cap = self.cap[low:high]\n","\n","    X1, X2, y = self.__get_data(batch_x_img, batch_x_cap)\n","    return [X1, X2], y\n","\n","  def __get_data(self, batch_img, batch_cap):\n","    X1, X2, y = list(), list(), list()\n","\n","    for img, cap in zip(batch_img, batch_cap):\n","      feature = img\n","      seq = cap\n","\n","      for i in range(1, len(seq)):\n","        in_seq, out_seq = seq[:i], seq[i]\n","        in_seq = pad_sequences([in_seq], maxlen=self.max_length, padding='post')[0]\n","        out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n","        X1.append(feature)\n","        X2.append(in_seq)\n","        y.append(out_seq)\n","\n","    X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","\n","    return X1, X2, y"],"metadata":{"id":"nsKAPJFsQNk2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### For list of captions per image"],"metadata":{"id":"6kVlVyB-n1uO"}},{"cell_type":"code","source":["class DataGenAugmented(Sequence):\n","\n","  def __init__(self, img_features, cap_encoded, batch_size, vocab_size, max_length):\n","\n","    self.img = img_features\n","    self.cap = cap_encoded\n","    self.batch_size = batch_size\n","    self.vocab_size = vocab_size\n","    self.max_length = max_length\n","    self.n = len(self.img)\n","\n","  def __len__(self):\n","    return self.n // self.batch_size\n","\n","  def __getitem__(self, idx):\n","    low = idx * self.batch_size\n","    high = min(low + self.batch_size, self.n)\n","    batch_x_img = self.img[low:high]\n","    batch_x_cap = self.cap[low:high]\n","\n","    X1, X2, y = self.__get_data(batch_x_img, batch_x_cap)\n","    return [X1, X2], y\n","\n","  def __get_data(self, batch_img, batch_cap):\n","    X1, X2, y = list(), list(), list()\n","\n","    for img, list_cap in zip(batch_img, batch_cap):\n","      feature = img\n","\n","      for cap in list_cap:\n","        seq = cap\n","\n","        for i in range(1, len(seq)):\n","          in_seq, out_seq = seq[:i], seq[i]\n","          in_seq = pad_sequences([in_seq], maxlen=self.max_length, padding='post')[0]\n","          out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n","          X1.append(feature)\n","          X2.append(in_seq)\n","          y.append(out_seq)\n","\n","    X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","\n","    return X1, X2, y"],"metadata":{"id":"HU-zAR0Fo79K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Definition"],"metadata":{"id":"j_BkqcHloAXl"}},{"cell_type":"markdown","source":["## Model Basic: VGG16 + LSTM"],"metadata":{"id":"ZzP-cUOmo78M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yc8VYesbf8j"},"outputs":[],"source":["def get_model_basic(voc_size, cap_length, emb_layer, folder_name):\n","  # image feature layers\n","  inputs1 = Input(shape=(4096,))\n","  d2 = Dense(256, activation='relu')(inputs1)\n","\n","  # sequence feature layers\n","  inputs2 = Input(shape=(cap_length,))\n","  emb = emb_layer(inputs2)\n","  lstm = LSTM(256)(emb)\n","\n","  # merge\n","  merge = add([d2, lstm])\n","  d3 = Dense(256, activation=\"relu\")(merge)\n","\n","  # softmax layer\n","  outputs = Dense(voc_size, activation=\"softmax\")(d3)\n","\n","  m = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\n","  # loss function\n","  m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n","\n","  plot_model(m, to_file=f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/plot.png\", show_shapes=True)\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/summary.txt\", \"w\") as f:\n","    with redirect_stdout(f):\n","      m.summary()\n","\n","  return m"]},{"cell_type":"markdown","source":["## Model Basic + Dropout"],"metadata":{"id":"guVwX93-PLgt"}},{"cell_type":"code","source":["def get_model_with_dropout(voc_size, cap_length, emb_layer, folder_name):\n","  # image feature layers\n","  inputs1 = Input(shape=(4096,))\n","  drop1 = Dropout(0.5)(inputs1)\n","  d2 = Dense(256, activation='relu')(drop1)\n","\n","  # sequence feature layers\n","  inputs2 = Input(shape=(cap_length,))\n","  emb = emb_layer(inputs2)\n","  lstm = LSTM(256)(emb)\n","\n","  # merge\n","  merge = add([d2, lstm])\n","  drop3 = Dropout(0.5)(merge)\n","  d3 = Dense(256, activation=\"relu\")(drop3)\n","\n","  # softmax layer\n","  outputs = Dense(voc_size, activation=\"softmax\")(d3)\n","\n","  m = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\n","  # loss function\n","  m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n","\n","  plot_model(m, to_file=f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/plot.png\")\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/summary.txt\", \"w\") as f:\n","    with redirect_stdout(f):\n","      m.summary()\n","\n","  return m"],"metadata":{"id":"gR5IQxiMPPj9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Basic + L1 Regularization\n"],"metadata":{"id":"lWiNJdsDm3kY"}},{"cell_type":"code","source":["def get_model_with_reg(voc_size, cap_length, emb_layer, folder_name):\n","  # image feature layers\n","  inputs1 = Input(shape=(4096,))\n","  d2 = Dense(256, activation='relu', kernel_regularizer='l1')(inputs1)\n","\n","  # sequence feature layers\n","  inputs2 = Input(shape=(cap_length,))\n","  emb = emb_layer(inputs2)\n","  lstm = LSTM(256)(emb)\n","\n","  # merge\n","  merge = add([d2, lstm])\n","  d3 = Dense(256, activation=\"relu\", kernel_regularizer='l1')(merge)\n","\n","  # softmax layer\n","  drop3 = Dropout(0.5)(d3)\n","  outputs = Dense(voc_size, activation=\"softmax\")(drop3)\n","\n","  m = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\n","  # loss function\n","  m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n","\n","  plot_model(m, to_file=f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/plot.png\")\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/summary.txt\", \"w\") as f:\n","    with redirect_stdout(f):\n","      m.summary()\n","\n","  return m"],"metadata":{"id":"XiOr-dLOm5Ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Basic + Dropout + Data Augmentation"],"metadata":{"id":"wEnnpzmx6pqk"}},{"cell_type":"code","source":["def get_model_with_dropout_aug(voc_size, cap_length, emb_layer, folder_name):\n","  # image feature layers\n","  inputs1 = Input(shape=(4096,))\n","  drop1 = Dropout(0.5)(inputs1)\n","  d2 = Dense(256, activation='relu')(drop1)\n","\n","  # sequence feature layers\n","  inputs2 = Input(shape=(cap_length,))\n","  emb = emb_layer(inputs2)\n","  lstm = LSTM(256)(emb)\n","\n","  # merge\n","  merge = add([d2, lstm])\n","  drop3 = Dropout(0.5)(merge)\n","  d3 = Dense(256, activation=\"relu\")(drop3)\n","\n","  # softmax layer\n","  outputs = Dense(voc_size, activation=\"softmax\")(d3)\n","\n","  m = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\n","  # loss function\n","  m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n","\n","  plot_model(m, to_file=f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/plot.png\")\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/summary.txt\", \"w\") as f:\n","    with redirect_stdout(f):\n","      m.summary()\n","\n","  return m"],"metadata":{"id":"lDeBUaz96tzf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Places + Dropout"],"metadata":{"id":"HTloo0eZ9loG"}},{"cell_type":"code","source":["def get_model_places(voc_size, cap_length, emb_layer, folder_name):\n","  # image feature layers\n","  inputs1 = Input(shape=(4096,))\n","  drop1 = Dropout(0.5)(inputs1)\n","  d2 = Dense(256, activation='relu')(drop1)\n","\n","  # sequence feature layers\n","  inputs2 = Input(shape=(cap_length,))\n","  emb = emb_layer(inputs2)\n","  lstm = LSTM(256)(emb)\n","\n","  # merge\n","  merge = add([d2, lstm])\n","  drop3 = Dropout(0.5)(merge)\n","  d3 = Dense(256, activation=\"relu\")(drop3)\n","\n","  # softmax layer\n","  outputs = Dense(voc_size, activation=\"softmax\")(d3)\n","\n","  m = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\n","  # loss function\n","  m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n","\n","  plot_model(m, to_file=f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/plot.png\")\n","\n","  with open(f\"{GDRIVE_WORKING_PATH}/models/{folder_name}/summary.txt\", \"w\") as f:\n","    with redirect_stdout(f):\n","      m.summary()\n","\n","  return m"],"metadata":{"id":"kc83mzfQ9wva"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Â Training"],"metadata":{"id":"RIn91iTboC_C"}},{"cell_type":"code","source":["def save_history_plot(h, path):\n","  plt.subplot(1, 2, 1)\n","  plt.plot(h.history['loss'])\n","  plt.plot(h.history['val_loss'])\n","  plt.ylim([0.0, 3.0])\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss')\n","  plt.legend(['train', 'valid'])\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(h.history['accuracy'])\n","  plt.plot(h.history['val_accuracy'])\n","  plt.ylim([0.5, 1.0])\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Accuracy')\n","  plt.legend(['train', 'valid'])\n","\n","  plt.savefig(f\"{path}/train_val_loss.png\")"],"metadata":{"id":"PxPe6-8A516n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["early_stop = tf.keras.callbacks.EarlyStopping(\n","  monitor=\"val_loss\",\n","  patience=10,\n","  restore_best_weights=True\n",")"],"metadata":{"id":"k-d56ledskkq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_on_epoch_end = tf.keras.callbacks.ModelCheckpoint(\n","  f\"{GDRIVE_WORKING_PATH}/models/dropout_aug/saved_model\",\n","  verbose = 0,\n","  save_freq=\"epoch\"\n",")"],"metadata":{"id":"GVmheKwTPS3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Basic: VGG16 + LSTM"],"metadata":{"id":"z9bZEwn9raDH"}},{"cell_type":"markdown","source":["The model stopped after 32 epochs, the weights are restored to best epoch"],"metadata":{"id":"R8E-UmT5OB3b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXIhTk_Bb_IH"},"outputs":[],"source":["if False:\n","  CAPTION_LENGTH = 73\n","  embedding_layer = get_embedding_layer()\n","  img_train, img_val, _ = get_features_vgg()\n","  caption_encoded_train, _, caption_encoded_val = get_vectorized_caption()\n","  model = get_model_basic(VOCAB_SIZE, CAPTION_LENGTH, embedding_layer, folder_name=\"basic\")\n","  g_train = DataGen(img_train, caption_encoded_train, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  g_val = DataGen(img_val, caption_encoded_val, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  history = model.fit(g_train, validation_data=g_val, epochs=50, verbose=1, callbacks=early_stop)\n","  save_history_plot(history, f\"{GDRIVE_WORKING_PATH}/models/basic\")\n","  model.save(f\"{GDRIVE_WORKING_PATH}/models/basic/saved_model\")"]},{"cell_type":"markdown","source":["## Model Basic + Dropout"],"metadata":{"id":"ZbOKKz4sDFQ1"}},{"cell_type":"code","source":["if False:\n","  CAPTION_LENGTH = 73\n","  embedding_layer = get_embedding_layer()\n","  img_train, img_val, _ = get_features_vgg()\n","  caption_encoded_train, _, caption_encoded_val = get_vectorized_caption()\n","  model = get_model_with_dropout(VOCAB_SIZE, CAPTION_LENGTH, embedding_layer, folder_name=\"plus_dropout\")\n","  g_train = DataGen(img_train, caption_encoded_train, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  g_val = DataGen(img_val, caption_encoded_val, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  history = model.fit(g_train, validation_data=g_val, epochs=50, verbose=1, callbacks=early_stop)\n","  save_history_plot(history, f\"{GDRIVE_WORKING_PATH}/models/plus_dropout\")\n","  model.save(f\"{GDRIVE_WORKING_PATH}/models/plus_dropout/saved_model\")"],"metadata":{"id":"Ast8hLjb_Lh1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Basic + L1 Regularization"],"metadata":{"id":"ukZ85uW0mktD"}},{"cell_type":"code","source":["if False:\n","  CAPTION_LENGTH = 73\n","  embedding_layer = get_embedding_layer()\n","  img_train, img_val, _ = get_features_vgg()\n","  caption_encoded_train, _, caption_encoded_val = get_vectorized_caption()\n","  model = get_model_with_reg(VOCAB_SIZE, CAPTION_LENGTH, embedding_layer, folder_name=\"plus_reg\")\n","  g_train = DataGen(img_train, caption_encoded_train, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  g_val = DataGen(img_val, caption_encoded_val, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  history = model.fit(g_train, validation_data=g_val, epochs=50, verbose=1, callbacks=early_stop)\n","  save_history_plot(history, f\"{GDRIVE_WORKING_PATH}/models/plus_reg\")\n","  model.save(f\"{GDRIVE_WORKING_PATH}/models/plus_reg/saved_model\")"],"metadata":{"id":"zbLJR4Jrsy48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Basic + Dropout + Data Augmentation"],"metadata":{"id":"19SdPXrknY1O"}},{"cell_type":"code","source":["if False:\n","  CAPTION_LENGTH = 76\n","  embedding_layer = get_embedding_layer_aug()\n","  img_train, img_val, _ = get_features_vgg()\n","  caption_encoded_train, caption_encoded_val = get_vectorized_caption_train_aug()\n","  model = get_model_with_dropout_aug(VOCAB_SIZE_AUG, CAPTION_LENGTH, embedding_layer, folder_name=\"dropout_aug\")\n","  g_train = DataGenAugmented(img_train, caption_encoded_train, 4, VOCAB_SIZE_AUG, CAPTION_LENGTH)\n","  g_val = DataGenAugmented(img_val, caption_encoded_val, 4, VOCAB_SIZE_AUG, CAPTION_LENGTH)\n","  history = model.fit(g_train, validation_data=g_val, epochs=50, verbose=1, callbacks=[save_on_epoch_end, early_stop])\n","  save_history_plot(history, f\"{GDRIVE_WORKING_PATH}/models/dropout_aug\")\n","  model.save(f\"{GDRIVE_WORKING_PATH}/models/dropout_aug/saved_model\")"],"metadata":{"id":"GnJjtWGonVYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Basic + Dropout with Places Weights"],"metadata":{"id":"I4ulHwdjirnm"}},{"cell_type":"code","source":["if False:\n","  CAPTION_LENGTH = 73\n","  embedding_layer = get_embedding_layer()\n","  img_train, img_val, _ = get_features_vgg_places()\n","  caption_encoded_train, _, caption_encoded_val = get_vectorized_caption()\n","\n","  # unable to convert to blob (missing img features)\n","  indexes = [train_ids.index('19575'), train_ids.index('19579'), train_ids.index('17975')]\n","  caption_encoded_train = list(caption_encoded_train)\n","  indexes = [8733, 10056, 13201]\n","  for index in sorted(indexes, reverse=True):\n","      del caption_encoded_train[index]\n","  caption_encoded_train = np.array(caption_encoded_train)\n","\n","  model = get_model_places(VOCAB_SIZE, CAPTION_LENGTH, embedding_layer, folder_name=\"plus_dropout_places\")\n","  g_train = DataGen(img_train, caption_encoded_train, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  g_val = DataGen(img_val, caption_encoded_val, 32, VOCAB_SIZE, CAPTION_LENGTH)\n","  history = model.fit(g_train, validation_data=g_val, epochs=50, verbose=1, callbacks=early_stop)\n","  save_history_plot(history, f\"{GDRIVE_WORKING_PATH}/models/plus_dropout_places\")\n","  model.save(f\"{GDRIVE_WORKING_PATH}/models/plus_dropout_places/saved_model\")"],"metadata":{"id":"gfjjh6T9i4w0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["zQMWYuEro8tb","jBO90z2WecaT","BwEDnS6pcJie","z8kjU4AJcRg-","CdiGZKEDcYqW","SrFAy9l_nw0p","6kVlVyB-n1uO","j_BkqcHloAXl","ZzP-cUOmo78M","guVwX93-PLgt","lWiNJdsDm3kY","wEnnpzmx6pqk","HTloo0eZ9loG","RIn91iTboC_C","z9bZEwn9raDH","ZbOKKz4sDFQ1","ukZ85uW0mktD","19SdPXrknY1O","I4ulHwdjirnm"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}